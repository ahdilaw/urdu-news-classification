\documentclass[sigconf]{acmart}
%%
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{acmlicensed}
\copyrightyear{2024}
\acmYear{2024}
\acmConference[CS 437 Machine Learning Group Project]{Fall 2024}{LUMS}

%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Classification of Urdu News Articles: Challenges and Insights in a Low-Resource Language}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Ahmed Wali}
\affiliation{%
}
\email{26100264@lums.edu.pk}



\author{Muhammad Ismail Humayun}
\affiliation{%
}
\email{25020267@lums.edu.pk}

\author{Hamza Sherjeel}
\affiliation{%
}
\email{26100361@lums.edu.pk}


\author{Yamsheen Saqib}
\affiliation{%
}
\email{26100379@lums.edu.pk}

 

\author{Shanzay Omer}
\affiliation{%
}
\email{26100202@lums.edu.pk}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Habibi Group 9}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
This study addresses the classification of Urdu news articles into five categories: Entertainment, Business, Sports, Science-Technology, and International. Leveraging a dataset scraped from seven major Urdu news websites, we employed a robust methodology encompassing data preprocessing, vocabulary curation, feature transformation, and the implementation of three machine learning models: K-Nearest Neighbors (KNN), Naïve Bayes, and a custom-designed neural network.\\

Our approach incorporates visualization techniques like PCA to analyze data distributions and ensures a comprehensive evaluation of model performance. We also present two variations of our models, demonstrating how they generalize to lengthy articles. Furthermore, we explore the limitations posed by the low-resource nature of the Urdu language, applying embeddings to address these challenges. However, our findings indicate that embeddings alone did not significantly enhance performance, underscoring the urgent need for further development and research in this domain.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%



%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Urdu, Classification, Machine Learning}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
The primary objective of this project was to develop an effective machine learning model for classifying Urdu articles into five predefined categories: Entertainment, Business, Sports, Science-Technology, and International. A corpus of 5841 articles was collected from various sources, processed, and analyzed to train and evaluate multiple machine learning models. The project involved extensive preprocessing, feature transformation, and experimentation with three machine learning models: K-Nearest Neighbors (KNN), Naïve Bayes, and a Neural Network implemented from scratch. This report details the methodology employed, the findings of the study, and an evaluation of the strengths and limitations of each model.

\section{Methodology}
The dataset consisted of 5841 Urdu articles sourced from ARY, Dunya, Express, Geo, Jang, BBC, and Dawn. After preprocessing and removing null entries, the corpus was reduced to 5841 articles. The vocabulary contained 51,074 unique Urdu words, which were sorted by frequency. Annotators manually labeled stopwords and non-meaningful words, supported by a list of 517 Urdu stopwords from Kaggle. The final vocabulary, comprising 49,869 meaningful words, was stored in a JSON file. A Cohen’s Kappa score of 0.664 confirmed the reliability of the annotations.

\subsection{Data Collection}
We collected data from diverse and credible Urdu news sources to build a robust and representative dataset for training and evaluation purposes. The training dataset, comprising data from ARY, Dunya News, Express, Geo, and Jang, totals 5943 rows with a combined size of 18.75 MB. The label distribution across five categories—Sports, Science-Technology, International, Entertainment, and Business—ensures balanced representation for effective model training. Additionally, two external test datasets were curated to simulate real-world scenarios: the Dawn dataset, which closely resembles the training distribution, and the BBC dataset, characterized by longer articles and a more varied distribution. The Dawn dataset contains 265 rows and is 1.34 MB in size, while the BBC dataset consists of 1177 rows and has a file size of 12.14 MB. Together, these datasets enable comprehensive evaluation and benchmarking of our models. For sources like ARY, where direct scraping was challenging, we utilized a custom script with randomized delays to collect data, ensuring authenticity and compliance. All datasets and corresponding scripts are included in the data/raw-datasets folder to ensure transparency and reproducibility.

\begin{table}[h]
    \caption{Summary of Training Dataset}
    \label{tab:training_summary}
    \begin{tabular}{|l|l|l|p{3cm}|}
        \hline
        \textbf{Source} & \textbf{File Size (MB)} & \textbf{Rows} & \textbf{Label Distribution} \\
        \hline
        ARY & 13.59 & 4526 & Science-Technology: 1220, Entertainment: 957, International: 910, Business: 720, Sports: 719 \\
        \hline
        Dunya News & 0.01 & 42 & International: 18, Sports: 12, Business: 6, Entertainment: 6 \\
        \hline
        Express & 0.99 & 500 & Entertainment: 100, Business: 100, Sports: 100, Science-Technology: 100, International: 100 \\
        \hline
        Geo & 0.72 & 379 & International: 199, Sports: 60, Entertainment: 60, Business: 60 \\
        \hline
        Jang & 3.41 & 496 & Sports: 496 \\
        \hline
    \end{tabular}
\end{table}

The combined training dataset consists of 5943 rows, distributed as shown in Table~\ref{tab:combined_summary}.

\begin{table}[h]
    \caption{Combined Training Dataset Summary}
    \label{tab:combined_summary}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Label} & \textbf{Count} \\
        \hline
        Sports & 1387 \\
        Science-Technology & 1320 \\
        International & 1227 \\
        Entertainment & 1123 \\
        Business & 886 \\
        \hline
        \textbf{Total Rows} & \textbf{5943} \\
        \hline
        \textbf{File Size (MB)} & \textbf{18.75} \\
        \hline
    \end{tabular}
\end{table}

We also created two external test datasets to simulate real-world scenarios:

\begin{itemize}
    \item \textbf{Dawn Dataset:} This dataset closely resembles the training dataset distribution. It contains 265 rows and is 1.34 MB in size. The label distribution is summarized in Table~\ref{tab:dawn_summary}.
    \item \textbf{BBC Dataset:} This dataset differs in distribution and contains longer articles, ideal for evaluating model generalization. It has 1177 rows and is 12.14 MB in size. The label distribution is summarized in Table~\ref{tab:bbc_summary}.
\end{itemize}

\begin{table}[h]
    \caption{Dawn Dataset Summary}
    \label{tab:dawn_summary}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Label} & \textbf{Count} \\
        \hline
        International & 170 \\
        Business & 69 \\
        Science-Technology & 23 \\
        Entertainment & 3 \\
        \hline
        \textbf{Total Rows} & \textbf{265} \\
        \textbf{File Size (MB)} & \textbf{1.34} \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{BBC Dataset Summary}
    \label{tab:bbc_summary}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Label} & \textbf{Count} \\
        \hline
        Entertainment & 240 \\
        Sports & 240 \\
        Science-Technology & 240 \\
        International & 234 \\
        Business & 223 \\
        \hline
        \textbf{Total Rows} & \textbf{1177} \\
        \textbf{File Size (MB)} & \textbf{12.14} \\
        \hline
    \end{tabular}
\end{table}

\paragraph{Scraping Strategy} For sources like ARY, where scraping directly was challenging, we first scraped article links and then scripted the data collection process with random delays to mimic human visitors. All datasets and code are included in the `data/raw-datasets` folder for reproducibility.

\subsection{Data Cleaning}

To clean the data, we followed a structured process involving multiple steps. The first step was to remove irrelevant or unnecessary data points, such as non-Urdu words, multiple spaces, and punctuation marks. After this, we processed the data to generate a clean, usable vocabulary for further analysis. Below is a summary of the data cleaning steps:

\subsubsection{Removing Non-Urdu Words and Multiple Spaces}

We started by cleaning the data and dropping missing values. We removed non-Urdu words, extra spaces, punctuation marks, and converted multiple spaces into single spaces.

\begin{table}[h]
    \caption{Data Cleaning Summary (Step 1)}
    \label{tab:data_cleaning_step1}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Action} & \textbf{Result} \\
        \hline
        Initial Row Count & 5943 \\
        \hline
        Rows After Dropping Missing Values & 5841 \\
        \hline
        Rows Dismissed Due to Missing Values & 102 \\
        \hline
        Final Row Count After Cleaning & 5841 \\
        \hline
        Unique Labels in 'gold\_label' & 5 \\
        \hline
        Counts per Label & 
        \begin{tabular}{@{}l@{}}
            Sports: 1375 \\
            Science-Technology: 1285 \\
            International: 1188 \\
            Entertainment: 1117 \\
            Business: 876
        \end{tabular} \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Vocabulary Creation}

Next, we built the vocabulary by processing the cleaned dataset. This step resulted in a vocabulary JSON file containing 51,071 unique Urdu words. The total number of words processed, including duplicates, was 2,164,726.

\begin{table}[h]
    \caption{Vocabulary Summary (Step 2)}
    \label{tab:vocabulary_summary}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Action} & \textbf{Result} \\
        \hline
        Unique Words in Vocabulary & 51,071 \\
        \hline
        Total Words Processed (Including Duplicates) & 2,164,726 \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Top 10 Most Common Words}

The vocabulary was then saved as a JSON file in the data/eda directory for later use.

\subsubsection{Stopwords Identification}

Next, we identified 517 stopwords from a Kaggle dataset for Urdu stopwords. The dataset used for stopword extraction can be found at [LINK A]. After applying the stopwords, we cleaned the dataset further, ensuring the text was free from common, non-informative words.

The final vocabulary contained 51,074 unique Urdu words after removing stopwords.

\subsection{Stopwords Annotation}

Since we suspected that there could be many other stopwords that might contribute to our dataset—considering the Kaggle dataset was limited to 517 words—we decided to manually annotate additional stopwords. To begin, we ordered the vocabulary by frequency and randomly removed 200 words to calculate Cohen's Kappa and introduce overlaps between the annotators.

We then divided the corpus amongst two annotators by assigning alternate words to each, asking them to flag words as either meaningful, not meaningful, or stopwords.

To make the annotation process easier and error-free, we developed a software tool for the annotators to quickly and accurately annotate the words. Below is a dummy image of the tool interface:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{abc.png} % Replace with actual image path
    \caption{Annotator's Software Tool for Stopwords Annotation}
    \label{fig:annotation_tool}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{def.png} % Replace with actual image path
    \caption{Annotator's Software Tool for Stopwords Annotation}
    \label{fig:annotation_tool}
\end{figure}

After gathering the annotations, we obtained the following distributions for the two annotators:

\begin{table}[h]
    \caption{Stopwords Annotation Distribution}
    \label{tab:stopwords_annotation}
    \begin{tabular}{}
        \hline
        \textbf{Annotator} & \textbf{Stopwords} & \textbf{Not Meaningful} & \textbf{Meaningful} \\
        \hline
        Annotator 1 & 242 & 145 & 24933 \\
        \hline
        Annotator 2 & 661 & 38 & 24936 \\
        \hline
    \end{tabular}
\end{table}

All the annotations can be found in the `data/eda` folder, which is included with this report.

\subsubsection{Estimating Cohen's Kappa for Inter-Annotator Agreement}

Since we had 200 overlapping words for annotation, we estimated Cohen's Kappa to assess inter-annotator agreement.

\textbf{Overlapping Words:} 200

\textbf{Annotator 1:} 199 words labeled as `0` (not meaningful), and 1 word labeled as `1` (meaningful).  
\textbf{Annotator 2:} 198 words labeled as `0` (not meaningful), and 2 words labeled as `1` (meaningful).

\textbf{Agreement and Disagreement in Overlap:}

- Agreement on `0`: Both annotators labeled 198 words as `0`.
- Agreement on `1`: Both annotators labeled 1 word as `1`.
- Disagreement: Annotator 2 labeled 1 additional word as `1`, but Annotator 1 labeled it as `0`.

\textbf{Agreement Breakdown:}

\[
\text{Agreement: } 198 \, (\text{on } 0) + 1 \, (\text{on } 1) = 199
\]
\[
\text{Total disagreements: } 200 - 199 = 1
\]

\textbf{1. Observed Agreement ($p_0$):}

\[
p_0 = \frac{\text{Number of agreements}}{\text{Total number of overlapping words}} = \frac{199}{200} = 0.995
\]

\textbf{2. Expected Agreement ($p_e$):}

To calculate the expected agreement, we use the marginal probabilities.

\textbf{Marginal Probabilities:}

For Annotator 1:
\[
p_1(0) = \frac{199}{200} = 0.995, \quad p_1(1) = \frac{1}{200} = 0.005
\]

For Annotator 2:
\[
p_2(0) = \frac{198}{200} = 0.99, \quad p_2(1) = \frac{2}{200} = 0.01
\]

\textbf{Expected Agreement ($p_e$):}
\[
p_e = (p_1(0) \cdot p_2(0)) + (p_1(1) \cdot p_2(1)) = (0.995 \cdot 0.99) + (0.005 \cdot 0.01) = 0.9851
\]

\textbf{3. Cohen’s Kappa ($\kappa$):}
\[
\kappa = \frac{p_0 - p_e}{1 - p_e} = \frac{0.995 - 0.9851}{1 - 0.9851} = \frac{0.0099}{0.0149} \approx 0.664
\]

\textbf{Summary of Results:}

- Observed Agreement ($p_0$): 0.995
- Expected Agreement ($p_e$): 0.9851
- Cohen’s Kappa ($\kappa$): 0.664

\textbf{Interpretation:}  
A Cohen’s Kappa value of 0.664 indicates substantial agreement between the annotators after accounting for chance.

\subsection{What does the data say about the model}

To visualize the dataset, we had to get into numbers. For that, we used the `fasttext.cc` embedding model called \texttt{cc.ur.300.vec.gz} in its vector form. It takes a sentence and converts it into a 300-dimensional embedding, preserving the semantic meaning of the sentence. After running the embeddings on the text, we then performed PCA and t-SNE analysis of the data.

\begin{figure}[h]
    \centering
    % Replace 'pca_image.png' with the actual filename of the PCA analysis image
    \includegraphics[width=0.5\textwidth]{pca1.png}
    \caption{PCA Analysis of the Dataset}
    \label{fig:pca_analysis}
\end{figure}

\begin{figure}[h]
    \centering
    % Replace 'tsne_image.png' with the actual filename of the t-SNE analysis image
    \includegraphics[width=0.5\textwidth]{pca2.png}
    \caption{t-SNE Analysis of the Dataset}
    \label{fig:tsne_analysis}
\end{figure}

As can be seen in the images, the PCA analysis shows visible features across which the words can be grouped together. The t-SNE analysis clearly shows clusters, thus giving us the opportunity to use nearest neighbor methods to cluster the words effectively. Later, we will demonstrate that nearest neighbor methods are actually the best ones for this task.

\subsection{Feature Transformation}
Two types of feature extraction methods were employed in this project. The first was a Count Vectorizer, which encoded each article into a sparse matrix by marking word occurrences. The second was a TF-IDF vectorizer, which captured the importance of words across documents. For neural networks, embeddings from FastText were used to represent words in a dense vector space.

\subsection{Model Selection}
Three machine learning models were implemented and evaluated. Each model utilized a distinct methodology for feature representation and classification.

\subsubsection{K-Nearest Neighbors (KNN)}
The KNN model employed a TF-IDF vectorizer, leveraging unigrams, bigrams, and trigrams for feature extraction. Cosine similarity was used as the distance metric. Hyperparameter tuning identified \(k = 4\) as the optimal value. The model excelled in classification tasks, particularly on validation data, but faced challenges in generalizing to datasets with a different distribution.

\subsubsection{Naïve Bayes}
The Naïve Bayes model utilized unigram features to estimate word probabilities within each category. Despite its simplicity, the model provided a strong baseline. It was effective for validation data but struggled with domain adaptation when tested on external datasets like BBC and Dawn.

\subsubsection{Neural Networks}
A custom neural network was developed using NumPy, demonstrating the potential of handcrafted models. It included \(_\) hidden layers and used \(_\) activation functions. Features were extracted using both Count Vectorizers and pre-trained FastText embeddings. While the neural network showed strong performance on the validation dataset, its computational cost and limitations in handling unseen data distributions were significant drawbacks.

\section{Findings}

The results revealed that KNN achieved the best overall performance on the Dawn test set, with an accuracy of \(_\). The neural network outperformed other models on the validation dataset, reaching an accuracy of \(_\). However, all models struggled with the BBC test set, which had a distribution differing significantly from the training data. Notably, the "International" category consistently exhibited lower precision and recall across all models, likely due to label overlap in the training data.

\subsection{KNN}

We have trained two models to see how well the KNN generalizes to the data. We first cleaned the data, implemented our own \texttt{CustomTfidfVectorizer} and the KNN class, and ran the analysis. The vectorizer took the sentences and converted them into sparse vectors in order of the vocabulary, and the \texttt{fit} method in the KNN performed the dot product to estimate the cosine distances. 

\textbf{Note:} The best \( k \) value thus established is \( k = 6 \), giving both the highest accuracy on the \textit{test set} at around 93\% and the least error.

\begin{figure}[h]
    \centering
    % Replace 'knn_accuracies.png' with the actual filename of the KNN accuracies image
    \includegraphics[width=0.5\textwidth]{knn1.png}
    \caption{KNN Accuracies on the Test Set}
    \label{fig:knn_accuracies}
\end{figure}

\begin{figure}[h]
    \centering
    % Replace 'knn_accuracies.png' with the actual filename of the KNN accuracies image
    \includegraphics[width=0.5\textwidth]{knn3.png}
    \caption{KNN Loss on the Test Set}
    \label{fig:knn_accuracies}
\end{figure}

\textbf{Best k value:} 6 with accuracy: 0.9229 on the internal test set.

\begin{figure}[h]
    \centering
    % Replace 'confusion_matrix.png' with the actual filename of the confusion matrix image
    \includegraphics[width=0.5\textwidth]{knn2.png}
    \caption{Confusion Matrix on the Internal Test Set}
    \label{fig:confusion_matrix}
\end{figure}

External test accuracy on the Dawn dataset: 0.8162. \\
External test accuracy on the BBC dataset: 0.6211. \\
These were only conducted to assess how well the model performed in relation to real-world data.

\subsection{Improving the KNN Model}

After all that, we trained the KNN model on the combined distributions of both datasets (7000 samples) and estimated the external test accuracy on the BBC dataset to be:

\textbf{Best k value:} 4 with accuracy: 0.8685.

\subsection{Naive Bayes}

We trained two variants of the Naive Bayes model: one using only the internal dataset and another trained on the combined dataset, which included both internal and external data. For both variants, we implemented a custom classifier from scratch.

\subsubsection{Internal Dataset Model}

For the Naive Bayes model trained solely on the internal dataset, the accuracy on the test dataset was found to be:

\[
\textbf{Accuracy on Test Dataset:} 0.9324
\]

\textbf{External Test Set Results:}
\begin{itemize}
    \item Dawn dataset: 0.8162
    \item BBC dataset: 0.61498
\end{itemize}

The model worked best when using unigrams as the features for training.

% \begin{figure}[h]
%     \centering
%     % Replace 'naive_bayes_internal.png' with the actual filename of the internal model performance image
%     \includegraphics[width=0.7\textwidth]{naive_bayes_internal.png}
%     \caption{Naive Bayes Performance on Internal Test Set}
%     \label{fig:naive_bayes_internal}
% \end{figure}

\subsubsection{Combined Dataset Model}

For the Naive Bayes model trained on the combined dataset, the accuracy on the test dataset was found to be:

\[
\textbf{Accuracy on Test Dataset:} 0.8782
\]

This model also worked best with unigrams as the features.

% \begin{figure}[h]
%     \centering
%     % Replace 'naive_bayes_combined.png' with the actual filename of the combined model performance image
%     \includegraphics[width=0.7\textwidth]{naive_bayes_combined.png}
%     \caption{Naive Bayes Performance on Combined Test Set}
%     \label{fig:naive_bayes_combined}
% \end{figure}

\subsection{Neural Networks}

We experimented with three flavors of the neural network model: one using the internal test data, one using the combined dataset, and one using embeddings on the internal test data in hopes of improving performance.

For both the internal and combined models, we used a simple count vectorizer. The architecture consisted of two hidden layers with 256 and 128 neurons each, along with an input and output layer. The batch size was set to 32, and the total number of epochs was 250. The model was built completely from scratch using numpy.

\subsubsection{Internal Dataset Model}

For the neural network trained on the internal dataset:

\begin{itemize}
    \item Epoch 240, Accuracy: 0.9679 (training accuracy)
    \item Neural Network Accuracy: 0.875 (suggests slight overfitting)
    \item Dawn dataset: Accuracy on new dataset: 0.7094
    \item BBC dataset: Accuracy on new dataset: 0.5322
\end{itemize}

% \begin{figure}[h]
%     \centering
%     % Replace 'neural_network_internal.png' with the actual filename for the internal dataset performance image
%     \includegraphics[width=0.7\textwidth]{neural_network_internal.png}
%     \caption{Neural Network Performance on Internal Test Set}
%     \label{fig:neural_network_internal}
% \end{figure}

\subsubsection{Combined Dataset Model}

For the neural network trained on the combined dataset:

\begin{itemize}
    \item Epoch 240, Accuracy: 0.9628 (training accuracy)
    \item Neural Network Accuracy: 0.8208
    \item BBC dataset: Accuracy on new dataset: 0.6847
    \item Dawn dataset: Accuracy on new dataset: 0.8803
\end{itemize}

% \begin{figure}[h]
%     \centering
%     % Replace 'neural_network_combined.png' with the actual filename for the combined dataset performance image
%     \includegraphics[width=0.7\textwidth]{neural_network_combined.png}
%     \caption{Neural Network Performance on Combined Test Set}
%     \label{fig:neural_network_combined}
% \end{figure}

\subsubsection{Model with Embeddings}

For the model using embeddings, we utilized the fasttext.cc embedding model that was used earlier in the PCA analysis. We hoped that these embeddings, which capture the semantic essence of words, would improve performance.

\begin{itemize}
    \item Epoch 490, F1 Score: 0.9919
    \item Neural Network Accuracy: 0.8965
    \item BBC dataset: Accuracy on new dataset: 0.5692
\end{itemize}

Despite using embeddings, the performance only showed a 2\% increase compared to the previous models. This result is further discussed in the limitations section.

% \begin{figure}[h]
%     \centering
%     % Replace 'neural_network_embeddings.png' with the actual filename for the embeddings model performance image
%     \includegraphics[width=0.7\textwidth]{neural_network_embeddings.png}
%     \caption{Neural Network Performance with Embeddings}
%     \label{fig:neural_network_embeddings}
% \end{figure}

\subsection{Other Models}

These models were tested to compare their performance against our models. The models included Random Forest, Logistic Regression, and Naive Bayes.

\subsubsection{Random Forest}

The Random Forest model was trained using a count vectorizer with 100 trees and maximum depth. On the training set, the model achieved an accuracy of 0.8349. However, it performed poorly on the BBC external dataset, with an accuracy of only 0.0127.

\begin{itemize}
    \item Random Forest Accuracy on Train Set: 0.8349
    \item Random Forest Accuracy on BBC External Dataset: 0.0127
\end{itemize}

\subsubsection{Logistic Regression}

The Logistic Regression model achieved an internal test accuracy of 0.9273, but it struggled to generalize to the external BBC dataset, where the accuracy dropped to below 12\%.

\begin{itemize}
    \item Logistic Regression Accuracy on Internal Test Set: 0.9273
    \item Logistic Regression Accuracy on BBC External Dataset: 0.12
\end{itemize}

\subsubsection{Naive Bayes}

Considering the performance of the other models, the winner is Naive Bayes. It achieved an accuracy of 87.5\% on the external test dataset and around 92\% accuracy on the internal test dataset, outperforming the other models.

\begin{itemize}
    \item Naive Bayes Accuracy on Internal Test Set: 0.92
    \item Naive Bayes Accuracy on External Test Set: 0.875
\end{itemize}


\section{Limitations and Conclusion}
This study demonstrated the effectiveness of machine learning models in classifying Urdu articles. While the models performed well on the validation and Dawn datasets, their accuracy dropped significantly on the BBC dataset, highlighting challenges in domain adaptation. The imbalanced dataset and potential mislabeling in categories, particularly "International," also contributed to lower performance.

Future work could explore advanced neural architectures such as transformers and transfer learning with pre-trained Urdu language models. Additionally, increasing dataset diversity and addressing label ambiguities could further enhance model performance.

\section{Future Considerations}

One of the key challenges we encountered during this project was that the currently available embeddings could not generalize well to the model. This indicates that state-of-the-art embedding models are not yet fully developed for the Urdu language. This limitation significantly impacted the model's performance, especially in terms of generalization. As a result, there is a need for immediate action to create and refine embedding models tailored specifically for resource-rich languages like Urdu. Such advancements are crucial to enhance the performance of machine learning models in these underrepresented languages.

Further such projects are required to validate the hypothesis.



%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\appendix
@misc{rtatman2019urdu,
    author = {T. Atman},
    title = {Urdu Stopwords List},
    year = {2019},
    howpublished = {\url{https://www.kaggle.com/datasets/rtatman/urdu-stopwords-list}},
    note = {Accessed: 8-Dec-2024}
}\\

@misc{bojanowski2017fasttext,
    author = {J. Bojanowski and P. Grave and A. Mikolov and A. Joulin and T. Mikolov},
    title = {Enriching Word Vectors with Subword Information},
    year = {2017},
    howpublished = {\url{https://fasttext.cc/docs/en/crawl-vectors.html}},
    note = {Accessed: 8-Dec-2024}
}

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
